{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtzLvXEGDl1QJyQW/iAwQj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishanth0233/INFO-5502/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "76o1et_zL2XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca711943-0751-4544-f213-95ce0723694a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting data from webpages, 0\n",
            "extracting data from webpages, 10\n",
            "extracting data from webpages, 20\n",
            "extracting data from webpages, 30\n",
            "extracting data from webpages, 40\n",
            "extracting data from webpages, 50\n",
            "extracting data from webpages, 60\n",
            "extracting data from webpages, 70\n",
            "extracting data from webpages, 80\n",
            "extracting data from webpages, 90\n",
            "extracting data from webpages, 100\n",
            "extracting data from webpages, 110\n",
            "extracting data from webpages, 120\n",
            "extracting data from webpages, 130\n",
            "extracting data from webpages, 140\n",
            "extracting data from webpages, 150\n",
            "extracting data from webpages, 160\n",
            "extracting data from webpages, 170\n",
            "extracting data from webpages, 180\n",
            "extracting data from webpages, 190\n",
            "extracting data from webpages, 200\n",
            "extracting data from webpages, 210\n",
            "extracting data from webpages, 220\n",
            "extracting data from webpages, 230\n",
            "extracting data from webpages, 240\n",
            "extracting data from webpages, 250\n",
            "extracting data from webpages, 260\n",
            "extracting data from webpages, 270\n",
            "extracting data from webpages, 280\n",
            "extracting data from webpages, 290\n",
            "extracting data from webpages, 300\n",
            "extracting data from webpages, 310\n",
            "extracting data from webpages, 320\n",
            "extracting data from webpages, 330\n",
            "extracting data from webpages, 340\n",
            "extracting data from webpages, 350\n",
            "extracting data from webpages, 360\n",
            "extracting data from webpages, 370\n",
            "extracting data from webpages, 380\n",
            "extracting data from webpages, 390\n",
            "extracting data from webpages, 400\n",
            "extracting data from webpages, 410\n",
            "extracting data from webpages, 420\n",
            "extracting data from webpages, 430\n",
            "extracting data from webpages, 440\n",
            "extracting data from webpages, 450\n",
            "extracting data from webpages, 460\n",
            "extracting data from webpages, 470\n",
            "extracting data from webpages, 480\n",
            "extracting data from webpages, 490\n",
            "extracting data from webpages, 500\n",
            "extracting data from webpages, 510\n",
            "extracting data from webpages, 520\n",
            "extracting data from webpages, 530\n",
            "extracting data from webpages, 540\n",
            "extracting data from webpages, 550\n",
            "extracting data from webpages, 560\n",
            "extracting data from webpages, 570\n",
            "extracting data from webpages, 580\n",
            "extracting data from webpages, 590\n",
            "extracting data from webpages, 600\n",
            "extracting data from webpages, 610\n",
            "extracting data from webpages, 620\n",
            "extracting data from webpages, 630\n",
            "extracting data from webpages, 640\n",
            "extracting data from webpages, 650\n",
            "extracting data from webpages, 660\n",
            "extracting data from webpages, 670\n",
            "extracting data from webpages, 680\n",
            "extracting data from webpages, 690\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "Check the csv file for whole data samples collected\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def extract(page):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'}\n",
        "    url = f'https://www.indeed.com/jobs?q=business%20analyst&l=United%20States&start={page}'\n",
        "    r = requests.get(url, headers)\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def transform(soup):\n",
        "    divs = soup.find_all('div', class_ = 'job_seen_beacon')\n",
        "    for item in divs:\n",
        "        title = item.find('h2').text.strip()\n",
        "        company = item.find('span', class_ = 'companyName').text.strip()\n",
        "  \n",
        "        try:\n",
        "            salary = item.find('div', class_ = 'attribute_snippet').text.strip()\n",
        "        except:\n",
        "            salary = ''\n",
        "        summary = item.find('div', class_ = 'job-snippet').text.strip().replace('\\n', '')\n",
        "\n",
        "        job = {\n",
        "            'title' : title,\n",
        "            'company' : company,\n",
        "            'salary' : salary,\n",
        "            'summary' : summary\n",
        "        }\n",
        "        joblist.append(job)\n",
        "    return         \n",
        "\n",
        "joblist = []\n",
        "\n",
        "for i in range(0,700,10):\n",
        "    print(f'extracting data from webpages, {i}')\n",
        "    c = extract(0)\n",
        "    transform(c)\n",
        "\n",
        "df = pd.DataFrame(joblist)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "df.to_csv('jobs.csv')\n",
        "\n",
        "print('Check the csv file for whole data samples collected')"
      ]
    }
  ]
}